{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install claudette"
      ],
      "metadata": {
        "id": "10gOcR4mzjop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import httpx\n",
        "from claudette import *\n",
        "from google.colab import userdata\n",
        "import re\n",
        "import json\n",
        "os.environ['ANTHROPIC_API_KEY'] = userdata.get('ANTHROPIC_API_KEY')"
      ],
      "metadata": {
        "id": "dj0uUwMur3Mj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/vishalbakshi/fastbook-benchmark/refs/heads/main/fastbook-benchmark.json\"\n",
        "response = httpx.get(url)\n",
        "data = response.json()\n",
        "len(data['questions']), data['questions'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAIfs0CUukBT",
        "outputId": "8e5d329c-201b-4a19-802f-63b79ab8266b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(191,\n",
              " {'chapter': 1,\n",
              "  'question_number': 1,\n",
              "  'question_text': 'Do you need these for deep learning?\\n\\n- Lots of math T / F\\n   - Lots of data T / F\\n   - Lots of expensive computers T / F\\n   - A PhD T / F',\n",
              "  'gold_standard_answer': '\"Lots of math - False\\nLots of data - False\\nLots of expensive computers - False\\nA PhD - False\"',\n",
              "  'answer_context': [{'answer_component': '\"Lots of math - False\\nLots of data - False\\nLots of expensive computers - False\\nA PhD - False\"',\n",
              "    'scoring_type': 'simple',\n",
              "    'context': ['```asciidoc\\n[[myths]]\\n.What you don\\'t need to do deep learning\\n[options=\"header\"]\\n|======\\n| Myth (don\\'t need) | Truth\\n| Lots of math | Just high school math is sufficient\\n| Lots of data | We\\'ve seen record-breaking results with <50 items of data\\n| Lots of expensive computers | You can get what you need for state of the art work for free\\n|======\\n```'],\n",
              "    'explicit_context': 'true',\n",
              "    'extraneous_answer': 'false'}],\n",
              "  'question_context': []})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX_76tFSqN_R",
        "outputId": "b944f541-0b94-4436-a0ee-0eeaf6ba0d11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<instruction>Decompose the original text into separate components without changing any language. Each component should be a unique concept that would require its own unique definition or explanation. Use the response template shown below. Extract text verbatim from the original text.</instruction>\n",
            "\n",
            "<original-text>\n",
            "\"In 1969, Marvin Minsky and Seymour Papert demonstrated in their book, “Perceptrons”, that a single layer of artificial neurons cannot learn simple, critical mathematical functions like XOR logic gate. While they subsequently demonstrated in the same book that additional layers can solve this problem, only the first insight was recognized, leading to the start of the first AI winter.\n",
            "\n",
            "In the 1980’s, models with two layers were being explored. Theoretically, it is possible to approximate any mathematical function using two layers of artificial neurons. However, in practices, these networks were too big and too slow. While it was demonstrated that adding additional layers improved performance, this insight was not acknowledged, and the second AI winter began. In this past decade, with increased data availability, and improvements in computer hardware (both in CPU performance but more importantly in GPU performance), neural networks are finally living up to its potential.\"\n",
            "<original-text>\n",
            "\n",
            "<additional-guidance>\n",
            "- When a sentence contains multiple distinct concepts connected by conjunctions or lists, consider whether each part would require different explanations. If so, separate them into separate components.\n",
            "- Separate structural facts from their descriptions. \"What exists\" vs \"what it contains/does\" are often distinct concepts and should be their own component.\n",
            "- Each component should represent a single concept that would need its own unique definition or context from source material.\n",
            "</additional-guidance>\n",
            "\n",
            "<response-template>\n",
            "<component id=\"1\">text goes here</component>\n",
            "<component id=\"2\">text goes here</component>\n",
            "<component id=\"...\">...</component>\n",
            "</response-template>\n",
            "\n",
            "<instruction>Decompose the original text into separate components without changing any language. Each component should be a unique concept that would require its own unique definition or explanation. Use the response template shown above. Extract text verbatim from the original text.</instruction>\n",
            "\n",
            "<additional-guidance>\n",
            "- When a sentence contains multiple distinct concepts connected by conjunctions or lists, consider whether each part would require different explanations. If so, separate them into separate components.\n",
            "- Separate structural facts from their descriptions. \"What exists\" vs \"what it contains/does\" are often distinct concepts and should be their own component.\n",
            "- Each component should represent a single concept that would need its own unique definition or context from source material.\n",
            "</additional-guidance>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"<instruction>Decompose the original text into separate components without changing any language. Each component should be a unique concept that would require its own unique definition or explanation. Use the response template shown below. Extract text verbatim from the original text.</instruction>\n",
        "\n",
        "<original-text>\n",
        "{text}\n",
        "<original-text>\n",
        "\n",
        "<additional-guidance>\n",
        "- When a sentence contains multiple distinct concepts connected by conjunctions or lists, consider whether each part would require different explanations. If so, separate them into separate components.\n",
        "- Separate structural facts from their descriptions. \"What exists\" vs \"what it contains/does\" are often distinct concepts and should be their own component.\n",
        "- Each component should represent a single concept that would need its own unique definition or context from source material.\n",
        "</additional-guidance>\n",
        "\n",
        "<response-template>\n",
        "<component id=\"1\">text goes here</component>\n",
        "<component id=\"2\">text goes here</component>\n",
        "<component id=\"...\">...</component>\n",
        "</response-template>\n",
        "\n",
        "<instruction>Decompose the original text into separate components without changing any language. Each component should be a unique concept that would require its own unique definition or explanation. Use the response template shown above. Extract text verbatim from the original text.</instruction>\n",
        "\n",
        "<additional-guidance>\n",
        "- When a sentence contains multiple distinct concepts connected by conjunctions or lists, consider whether each part would require different explanations. If so, separate them into separate components.\n",
        "- Separate structural facts from their descriptions. \"What exists\" vs \"what it contains/does\" are often distinct concepts and should be their own component.\n",
        "- Each component should represent a single concept that would need its own unique definition or context from source material.\n",
        "</additional-guidance>\n",
        "\"\"\"\n",
        "\n",
        "print(prompt.format(text=data['questions'][4]['gold_standard_answer']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"claude-3-5-haiku-20241022\"\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XWjR0A0XzXXf",
        "outputId": "b3dc4171-a0db-4d5c-dcea-ee83e55b4c3c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'claude-3-5-haiku-20241022'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n",
        "chat(\"I'm Vishal\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "enH2iQRxzw7b",
        "outputId": "3de122c8-e758-4b6b-93d3-da3fb2b25625"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Message(id='msg_01SGki33UFx3zsArEhFXRfgZ', content=[TextBlock(citations=None, text='Hi Vishal! How can I help you today?', type='text')], model='claude-3-5-haiku-20241022', role='assistant', stop_reason='end_turn', stop_sequence=None, type='message', usage=In: 21; Out: 15; Cache create: 0; Cache read: 0; Total Tokens: 36; Server tool use (web search requests): 0)"
            ],
            "text/markdown": "Hi Vishal! How can I help you today?\n\n<details>\n\n- id: `msg_01SGki33UFx3zsArEhFXRfgZ`\n- content: `[{'citations': None, 'text': 'Hi Vishal! How can I help you today?', 'type': 'text'}]`\n- model: `claude-3-5-haiku-20241022`\n- role: `assistant`\n- stop_reason: `end_turn`\n- stop_sequence: `None`\n- type: `message`\n- usage: `{'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 21, 'output_tokens': 15, 'server_tool_use': None, 'service_tier': 'standard'}`\n\n</details>"
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ch4 = [item for item in data['questions'] if item['chapter'] == 4]\n",
        "ch4[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNKWjwgUBBsp",
        "outputId": "3a3bc91d-bf04-4504-9fab-f1c4a854e7af"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chapter': 4,\n",
              " 'question_number': 2,\n",
              " 'question_text': '\"How are the files and folders in the MNIST_SAMPLE dataset structured? Why?\"',\n",
              " 'gold_standard_answer': '\"There are two subfolders, train and valid, the former contains the data for model training, the latter contains the data for validating model performance after each training step. Evaluating the model on the validation set serves two purposes: a) to report a human-interpretable metric such as accuracy (in contrast to the often abstract loss functions used for training), b) to facilitate the detection of overfitting by evaluating the model on a dataset it hasn’t been trained on (in short, an overfitting model performs increasingly well on the training set but decreasingly so on the validation set). Of course, every practicioner could generate their own train/validation-split of the data. Public datasets are usually pre-split to simplifiy comparing results between implementations/publications.\\n\\nEach subfolder has two subsubfolders 3 and 7 which contain the .jpg files for the respective class of images. This is a common way of organizing datasets comprised of pictures. For the full MNIST dataset there are 10 subsubfolders, one for the images for each digit.\"',\n",
              " 'answer_context': [{'answer_component': 'There are two subfolders, train and valid, the former contains the data for model training, the latter contains the data for validating model performance after each training step',\n",
              "   'scoring_type': 'simple',\n",
              "   'context': [\"The MNIST dataset follows a common layout for machine learning datasets: separate folders for the training set and the validation set (and/or test set). Let's see what's inside the training set:\"],\n",
              "   'explicit_context': 'true',\n",
              "   'extraneous_answer': 'false'},\n",
              "  {'answer_component': 'Each subfolder has two subsubfolders 3 and 7',\n",
              "   'scoring_type': 'simple',\n",
              "   'context': [\"There's a folder of 3s, and a folder of 7s\"],\n",
              "   'explicit_context': 'true',\n",
              "   'extraneous_answer': 'false'},\n",
              "  {'answer_component': 'which contain the .jpg files for the respective class of images',\n",
              "   'scoring_type': 'simple',\n",
              "   'context': [\"```python\\nthrees = (path/'train'/'3').ls().sorted()\\nsevens = (path/'train'/'7').ls().sorted()\\nthrees\\n```\\nOutput:\\n(#6131) [Path('train/3/10.png'),Path('train/3/10000.png'),Path('train/3/10011.png'),Path('train/3/10031.png'),Path('train/3/10034.png'),Path('train/3/10042.png'),Path('train/3/10052.png'),Path('train/3/1007.png'),Path('train/3/10074.png'),Path('train/3/10091.png')...]\",\n",
              "    \"As we might expect, it's full of image files\"],\n",
              "   'explicit_context': 'true',\n",
              "   'extraneous_answer': 'false'},\n",
              "  {'answer_component': 'Evaluating the model on the validation set',\n",
              "   'scoring_type': 'simple',\n",
              "   'context': [\"As we've discussed, we want to calculate our metric over a *validation set*\"],\n",
              "   'explicit_context': 'true',\n",
              "   'extraneous_answer': 'true'},\n",
              "  {'answer_component': 'facilitate the detection of overfitting by evaluating the model on a dataset it hasn’t been trained on',\n",
              "   'scoring_type': 'simple',\n",
              "   'context': [\"As we've discussed, we want to calculate our metric over a *validation set*. This is so that we don't inadvertently overfit—that is, train a model to work well only on our training data\"],\n",
              "   'explicit_context': 'true',\n",
              "   'extraneous_answer': 'true'},\n",
              "  {'answer_component': 'a) to report a human-interpretable metric such as accuracy (in contrast to the often abstract loss functions used for training)',\n",
              "   'scoring_type': 'simple',\n",
              "   'context': ['The key difference is that the metric is to drive human understanding and the loss is to drive automated learning'],\n",
              "   'explicit_context': 'true',\n",
              "   'extraneous_answer': 'true'},\n",
              "  {'answer_component': 'Public datasets are usually pre-split to simplifiy comparing results between implementations/publications',\n",
              "   'scoring_type': 'simple',\n",
              "   'context': [],\n",
              "   'explicit_context': 'false',\n",
              "   'extraneous_answer': 'true'},\n",
              "  {'answer_component': 'For the full MNIST dataset there are 10 subsubfolders, one for the images for each digit',\n",
              "   'scoring_type': 'simple',\n",
              "   'context': [],\n",
              "   'explicit_context': 'false',\n",
              "   'extraneous_answer': 'true'}],\n",
              " 'question_context': []}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ch4[5]['gold_standard_answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "K1OdeabR2aKw",
        "outputId": "04506b93-d994-42f1-c0de-85a7123861c0"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"Root mean square error (RMSE), also called the L2 norm, and mean absolute difference (MAE), also called the L1 norm, are two commonly used methods of measuring “distance”. Simple differences do not work because some difference are positive and others are negative, canceling each other out. Therefore, a function that focuses on the magnitudes of the differences is needed to properly measure distances. The simplest would be to add the absolute values of the differences, which is what MAE is. RMSE takes the mean of the square (makes everything positive) and then takes the square root (undoes squaring).\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n",
        "res = chat(prompt.format(text=ch4[5]['gold_standard_answer']))\n",
        "print(res.content[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PSpyzkQzzXM",
        "outputId": "5c6f17f3-7171-4797-9778-264d29f2bda1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<component id=\"1\">Root mean square error (RMSE), also called the L2 norm</component>\n",
            "<component id=\"2\">mean absolute difference (MAE), also called the L1 norm</component>\n",
            "<component id=\"3\">two commonly used methods of measuring \"distance\"</component>\n",
            "<component id=\"4\">Simple differences do not work because some difference are positive and others are negative, canceling each other out</component>\n",
            "<component id=\"5\">a function that focuses on the magnitudes of the differences is needed to properly measure distances</component>\n",
            "<component id=\"6\">The simplest would be to add the absolute values of the differences, which is what MAE is</component>\n",
            "<component id=\"7\">RMSE takes the mean of the square (makes everything positive) and then takes the square root (undoes squaring)</component>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for o in re.finditer(r'<component id=\"\\d+\">(.*?)</component>', res.content[0].text): print(o)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo60bQnN2d0R",
        "outputId": "b9db2f31-434b-4a47-f038-be20f707c9ba"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<re.Match object; span=(0, 84), match='<component id=\"1\">Root mean square error (RMSE), >\n",
            "<re.Match object; span=(85, 170), match='<component id=\"2\">mean absolute difference (MAE),>\n",
            "<re.Match object; span=(171, 250), match='<component id=\"3\">two commonly used methods of me>\n",
            "<re.Match object; span=(251, 398), match='<component id=\"4\">Simple differences do not work >\n",
            "<re.Match object; span=(399, 529), match='<component id=\"5\">a function that focuses on the >\n",
            "<re.Match object; span=(530, 649), match='<component id=\"6\">The simplest would be to add th>\n",
            "<re.Match object; span=(650, 790), match='<component id=\"7\">RMSE takes the mean of the squa>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[match.group(1) for match in re.finditer(r'<component id=\"\\d+\">(.*?)</component>', res.content[0].text)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ42eT3633Oj",
        "outputId": "c8ddfb07-da6b-4764-d7a9-94c9a6bdd08e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Root mean square error (RMSE), also called the L2 norm',\n",
              " 'mean absolute difference (MAE), also called the L1 norm',\n",
              " 'two commonly used methods of measuring \"distance\"',\n",
              " 'Simple differences do not work because some difference are positive and others are negative, canceling each other out',\n",
              " 'a function that focuses on the magnitudes of the differences is needed to properly measure distances',\n",
              " 'The simplest would be to add the absolute values of the differences, which is what MAE is',\n",
              " 'RMSE takes the mean of the square (makes everything positive) and then takes the square root (undoes squaring)']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_components = []\n",
        "for o in ch4[:5]:\n",
        "    chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n",
        "    res = chat(prompt.format(text=o['gold_standard_answer']))\n",
        "    components = [match.group(1) for match in re.finditer(r'<component id=\"\\d+\">(.*?)</component>', res.content[0].text)]\n",
        "    all_components.append(components)"
      ],
      "metadata": {
        "id": "Yg7n8AH633u2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_components"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3sKHrac4CV7",
        "outputId": "87285a76-dc77-4a7d-a2dc-f2a21136a401"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['There are two subfolders, train and valid',\n",
              "  'the former contains the data for model training',\n",
              "  'the latter contains the data for validating model performance after each training step',\n",
              "  'Evaluating the model on the validation set serves two purposes: a) to report a human-interpretable metric such as accuracy (in contrast to the often abstract loss functions used for training)',\n",
              "  \"b) to facilitate the detection of overfitting by evaluating the model on a dataset it hasn't been trained on (in short, an overfitting model performs increasingly well on the training set but decreasingly so on the validation set)\",\n",
              "  'every practicioner could generate their own train/validation-split of the data',\n",
              "  'Public datasets are usually pre-split to simplifiy comparing results between implementations/publications',\n",
              "  'Each subfolder has two subsubfolders 3 and 7 which contain the .jpg files for the respective class of images',\n",
              "  'This is a common way of organizing datasets comprised of pictures',\n",
              "  'For the full MNIST dataset there are 10 subsubfolders, one for the images for each digit'],\n",
              " ['\"In the \"pixel similarity\" approach, we generate an archetype for each class we want to identify.\"',\n",
              "  '\"In our case, we want to distinguish images of 3\\'s from images of 7\\'s.\"',\n",
              "  '\"We define the archetypical 3 as the pixel-wise mean value of all 3\\'s in the training set.\"',\n",
              "  '\"Analoguously for the 7\\'s.\"',\n",
              "  '\"You can visualize the two archetypes and see that they are in fact blurred versions of the numbers they represent.\"',\n",
              "  '\"In order to tell if a previously unseen image is a 3 or a 7, we calculate its distance to the two archetypes (here: mean pixel-wise absolute difference).\"',\n",
              "  '\"We say the new image is a 3 if its distance to the archetypical 3 is lower than two the archetypical 7.\"'],\n",
              " ['Lists (arrays in other programming languages) are often generated using a for-loop.',\n",
              "  'A list comprehension is a Pythonic way of condensing the creation of a list using a for-loop into a single expression.',\n",
              "  'List comprehensions will also often include if clauses for filtering.'],\n",
              " ['The rank of a tensor is the number of dimensions it has.',\n",
              "  'An easy way to identify the rank is the number of indices you would need to reference a number within a tensor.',\n",
              "  'A scalar can be represented as a tensor of rank 0 (no index)',\n",
              "  'A vector can be represented as a tensor of rank 1 (one index, e.g., v[i])',\n",
              "  'A matrix can be represented as a tensor of rank 2 (two indices, e.g., a[i,j])',\n",
              "  'A tensor of rank 3 is a cuboid or a \"stack of matrices\" (three indices, e.g., b[i,j,k])',\n",
              "  'The rank of a tensor is independent of its shape or dimensionality, e.g., a tensor of shape 2x2x2 and a tensor of shape 3x5x7 both have rank 3.',\n",
              "  'The term \"rank\" has different meanings in the context of tensors and matrices (where it refers to the number of linearly independent column vectors).'],\n",
              " ['Rank is the number of axes or dimensions in a tensor;',\n",
              "  'shape is the size of each axis of a tensor.',\n",
              "  \"The length of a tensor's shape is its rank.\",\n",
              "  'So if we have the images of the 3 folder from the MINST_SAMPLE dataset in a tensor called stacked_threes and we find its shape like this.',\n",
              "  'We just need to find its length to know its rank. This is done as follows.',\n",
              "  \"You can also get a tensor's rank directly with ndim .\"]]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluations = []\n",
        "for i,o in enumerate(ch4[:5]):\n",
        "    item = {\n",
        "        \"chapter\": o[\"chapter\"],\n",
        "        \"question_number\": o[\"question_number\"],\n",
        "        \"question_text\": o[\"question_text\"],\n",
        "        \"gold_standard_answer\": o[\"gold_standard_answer\"],\n",
        "        \"ground_truth_components\": [x['answer_component'] for x in o['answer_context']],\n",
        "        \"haiku_components\": all_components[i]\n",
        "        }\n",
        "    evaluations.append(item)"
      ],
      "metadata": {
        "id": "oSgOZ4zK8FFi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsS9qbpw8JGM",
        "outputId": "2ff3270b-7ae3-477d-a10d-6f352baa2e22"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'chapter': 4,\n",
              "  'question_number': 2,\n",
              "  'question_text': '\"How are the files and folders in the MNIST_SAMPLE dataset structured? Why?\"',\n",
              "  'gold_standard_answer': '\"There are two subfolders, train and valid, the former contains the data for model training, the latter contains the data for validating model performance after each training step. Evaluating the model on the validation set serves two purposes: a) to report a human-interpretable metric such as accuracy (in contrast to the often abstract loss functions used for training), b) to facilitate the detection of overfitting by evaluating the model on a dataset it hasn’t been trained on (in short, an overfitting model performs increasingly well on the training set but decreasingly so on the validation set). Of course, every practicioner could generate their own train/validation-split of the data. Public datasets are usually pre-split to simplifiy comparing results between implementations/publications.\\n\\nEach subfolder has two subsubfolders 3 and 7 which contain the .jpg files for the respective class of images. This is a common way of organizing datasets comprised of pictures. For the full MNIST dataset there are 10 subsubfolders, one for the images for each digit.\"',\n",
              "  'ground_truth_components': ['There are two subfolders, train and valid, the former contains the data for model training, the latter contains the data for validating model performance after each training step',\n",
              "   'Each subfolder has two subsubfolders 3 and 7',\n",
              "   'which contain the .jpg files for the respective class of images',\n",
              "   'Evaluating the model on the validation set',\n",
              "   'facilitate the detection of overfitting by evaluating the model on a dataset it hasn’t been trained on',\n",
              "   'a) to report a human-interpretable metric such as accuracy (in contrast to the often abstract loss functions used for training)',\n",
              "   'Public datasets are usually pre-split to simplifiy comparing results between implementations/publications',\n",
              "   'For the full MNIST dataset there are 10 subsubfolders, one for the images for each digit'],\n",
              "  'haiku_components': ['There are two subfolders, train and valid',\n",
              "   'the former contains the data for model training',\n",
              "   'the latter contains the data for validating model performance after each training step',\n",
              "   'Evaluating the model on the validation set serves two purposes: a) to report a human-interpretable metric such as accuracy (in contrast to the often abstract loss functions used for training)',\n",
              "   \"b) to facilitate the detection of overfitting by evaluating the model on a dataset it hasn't been trained on (in short, an overfitting model performs increasingly well on the training set but decreasingly so on the validation set)\",\n",
              "   'every practicioner could generate their own train/validation-split of the data',\n",
              "   'Public datasets are usually pre-split to simplifiy comparing results between implementations/publications',\n",
              "   'Each subfolder has two subsubfolders 3 and 7 which contain the .jpg files for the respective class of images',\n",
              "   'This is a common way of organizing datasets comprised of pictures',\n",
              "   'For the full MNIST dataset there are 10 subsubfolders, one for the images for each digit']},\n",
              " {'chapter': 4,\n",
              "  'question_number': 3,\n",
              "  'question_text': '\"Explain how the \"pixel similarity\" approach to classifying digits works.\"',\n",
              "  'gold_standard_answer': '\"In the “pixel similarity” approach, we generate an archetype for each class we want to identify. In our case, we want to distinguish images of 3’s from images of 7’s. We define the archetypical 3 as the pixel-wise mean value of all 3’s in the training set. Analoguously for the 7’s. You can visualize the two archetypes and see that they are in fact blurred versions of the numbers they represent.\\nIn order to tell if a previously unseen image is a 3 or a 7, we calculate its distance to the two archetypes (here: mean pixel-wise absolute difference). We say the new image is a 3 if its distance to the archetypical 3 is lower than two the archetypical 7.\"',\n",
              "  'ground_truth_components': ['We define the archetypical 3 as the pixel-wise mean value of all 3’s in the training set. Analoguously for the 7’s. You can visualize the two archetypes and see that they are in fact blurred versions of the numbers they represent.\\nIn order to tell if a previously unseen image is a 3 or a 7, we calculate its distance to the two archetypes (here: mean pixel-wise absolute difference). We say the new image is a 3 if its distance to the archetypical 3 is lower than two the archetypical 7'],\n",
              "  'haiku_components': ['\"In the \"pixel similarity\" approach, we generate an archetype for each class we want to identify.\"',\n",
              "   '\"In our case, we want to distinguish images of 3\\'s from images of 7\\'s.\"',\n",
              "   '\"We define the archetypical 3 as the pixel-wise mean value of all 3\\'s in the training set.\"',\n",
              "   '\"Analoguously for the 7\\'s.\"',\n",
              "   '\"You can visualize the two archetypes and see that they are in fact blurred versions of the numbers they represent.\"',\n",
              "   '\"In order to tell if a previously unseen image is a 3 or a 7, we calculate its distance to the two archetypes (here: mean pixel-wise absolute difference).\"',\n",
              "   '\"We say the new image is a 3 if its distance to the archetypical 3 is lower than two the archetypical 7.\"']},\n",
              " {'chapter': 4,\n",
              "  'question_number': 4,\n",
              "  'question_text': '\"What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\"',\n",
              "  'gold_standard_answer': '\"Lists (arrays in other programming languages) are often generated using a for-loop. A list comprehension is a Pythonic way of condensing the creation of a list using a for-loop into a single expression. List comprehensions will also often include if clauses for filtering.\\n\\nlst_in = range(10)\\nlst_out = [2*el for el in lst_in if el%2==1]\\n# is equivalent to:\\nlst_out = []\\nfor el in lst_in:\\n   if el%2==1:\\n       lst_out.append(2*el)\"',\n",
              "  'ground_truth_components': ['\"Lists (arrays in other programming languages) are often generated using a for-loop. A list comprehension is a Pythonic way of condensing the creation of a list using a for-loop into a single expression. List comprehensions will also often include if clauses for filtering.\\n\\nlst_in = range(10)\\nlst_out = [2*el for el in lst_in if el%2==1]\\n# is equivalent to:\\nlst_out = []\\nfor el in lst_in:\\n   if el%2==1:\\n       lst_out.append(2*el)\"'],\n",
              "  'haiku_components': ['Lists (arrays in other programming languages) are often generated using a for-loop.',\n",
              "   'A list comprehension is a Pythonic way of condensing the creation of a list using a for-loop into a single expression.',\n",
              "   'List comprehensions will also often include if clauses for filtering.']},\n",
              " {'chapter': 4,\n",
              "  'question_number': 5,\n",
              "  'question_text': '\"What is a \"rank-3 tensor\"?\"',\n",
              "  'gold_standard_answer': '\"The rank of a tensor is the number of dimensions it has. An easy way to identify the rank is the number of indices you would need to reference a number within a tensor. A scalar can be represented as a tensor of rank 0 (no index), a vector can be represented as a tensor of rank 1 (one index, e.g., v[i]), a matrix can be represented as a tensor of rank 2 (two indices, e.g., a[i,j]), and a tensor of rank 3 is a cuboid or a “stack of matrices” (three indices, e.g., b[i,j,k]). In particular, the rank of a tensor is independent of its shape or dimensionality, e.g., a tensor of shape 2x2x2 and a tensor of shape 3x5x7 both have rank 3.\\nNote that the term “rank” has different meanings in the context of tensors and matrices (where it refers to the number of linearly independent column vectors).\"',\n",
              "  'ground_truth_components': ['The rank of a tensor is the number of dimensions it has',\n",
              "   'An easy way to identify the rank is the number of indices you would need to reference a number within a tensor',\n",
              "   'A scalar can be represented as a tensor of rank 0 (no index), a vector can be represented as a tensor of rank 1 (one index, e.g., v[i]), a matrix can be represented as a tensor of rank 2 (two indices, e.g., a[i,j])',\n",
              "   'and a tensor of rank 3 is a cuboid or a “stack of matrices”',\n",
              "   'In particular, the rank of a tensor is independent of its shape or dimensionality, e.g., a tensor of shape 2x2x2 and a tensor of shape 3x5x7 both have rank 3',\n",
              "   'Note that the term “rank” has different meanings in the context of tensors and matrices (where it refers to the number of linearly independent column vectors)'],\n",
              "  'haiku_components': ['The rank of a tensor is the number of dimensions it has.',\n",
              "   'An easy way to identify the rank is the number of indices you would need to reference a number within a tensor.',\n",
              "   'A scalar can be represented as a tensor of rank 0 (no index)',\n",
              "   'A vector can be represented as a tensor of rank 1 (one index, e.g., v[i])',\n",
              "   'A matrix can be represented as a tensor of rank 2 (two indices, e.g., a[i,j])',\n",
              "   'A tensor of rank 3 is a cuboid or a \"stack of matrices\" (three indices, e.g., b[i,j,k])',\n",
              "   'The rank of a tensor is independent of its shape or dimensionality, e.g., a tensor of shape 2x2x2 and a tensor of shape 3x5x7 both have rank 3.',\n",
              "   'The term \"rank\" has different meanings in the context of tensors and matrices (where it refers to the number of linearly independent column vectors).']},\n",
              " {'chapter': 4,\n",
              "  'question_number': 6,\n",
              "  'question_text': '\"What is the difference between tensor rank and shape? How do you get the rank from the shape?\"',\n",
              "  'gold_standard_answer': '\"Rank is the number of axes or dimensions in a tensor; shape is the size of each axis of a tensor.\\n\\nThe length of a tensor’s shape is its rank.\\n\\nSo if we have the images of the 3 folder from the MINST_SAMPLE dataset in a tensor called stacked_threes and we find its shape like this.\\n\\nIn [ ]: stacked_threes.shape\\nOut[ ]: torch.Size([6131, 28, 28])\\nWe just need to find its length to know its rank. This is done as follows.\\n\\nIn [ ]: len(stacked_threes.shape)\\nOut[ ]: 3\\nYou can also get a tensor’s rank directly with ndim .\\n\\nIn [ ]: stacked_threes.ndim\\nOut[ ]: 3\"',\n",
              "  'ground_truth_components': ['Rank is the number of axes or dimensions in a tensor',\n",
              "   'shape is the size of each axis of a tensor',\n",
              "   'The length of a tensor’s shape is its rank',\n",
              "   'So if we have the images of the 3 folder from the MINST_SAMPLE dataset in a tensor called stacked_threes and we find its shape like this.\\n\\nIn [ ]: stacked_threes.shape\\nOut[ ]: torch.Size([6131, 28, 28])\\n\"',\n",
              "   'In [ ]: stacked_threes.ndim\\nOut[ ]: 3',\n",
              "   'You can also get a tensor’s rank directly with ndim'],\n",
              "  'haiku_components': ['Rank is the number of axes or dimensions in a tensor;',\n",
              "   'shape is the size of each axis of a tensor.',\n",
              "   \"The length of a tensor's shape is its rank.\",\n",
              "   'So if we have the images of the 3 folder from the MINST_SAMPLE dataset in a tensor called stacked_threes and we find its shape like this.',\n",
              "   'We just need to find its length to know its rank. This is done as follows.',\n",
              "   \"You can also get a tensor's rank directly with ndim .\"]}]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('evals.json', 'w') as f: json.dump(evaluations, f, indent=2)"
      ],
      "metadata": {
        "id": "_jyVM-sx8Jgh"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_components = []\n",
        "for o in ch4:\n",
        "    chat = Chat(model, sp=\"\"\"You are a helpful and concise assistant.\"\"\")\n",
        "    res = chat(prompt.format(text=o['gold_standard_answer']))\n",
        "    components = [match.group(1) for match in re.finditer(r'<component id=\"\\d+\">(.*?)</component>', res.content[0].text)]\n",
        "    all_components.append(components)"
      ],
      "metadata": {
        "id": "bKyfeAp99ABH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluations = []\n",
        "for i,o in enumerate(ch4):\n",
        "    item = {\n",
        "        \"chapter\": o[\"chapter\"],\n",
        "        \"question_number\": o[\"question_number\"],\n",
        "        \"question_text\": o[\"question_text\"],\n",
        "        \"gold_standard_answer\": o[\"gold_standard_answer\"],\n",
        "        \"ground_truth_components\": [x['answer_component'] for x in o['answer_context']],\n",
        "        \"haiku_components\": all_components[i]\n",
        "        }\n",
        "    evaluations.append(item)"
      ],
      "metadata": {
        "id": "yyXH84aaC-H0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('evals.json', 'w') as f: json.dump(evaluations, f, indent=2)"
      ],
      "metadata": {
        "id": "nlJHRqFADnox"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}