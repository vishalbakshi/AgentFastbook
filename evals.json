[
  {
    "chapter": 4,
    "question_number": 2,
    "question_text": "\"How are the files and folders in the MNIST_SAMPLE dataset structured? Why?\"",
    "gold_standard_answer": "\"There are two subfolders, train and valid, the former contains the data for model training, the latter contains the data for validating model performance after each training step. Evaluating the model on the validation set serves two purposes: a) to report a human-interpretable metric such as accuracy (in contrast to the often abstract loss functions used for training), b) to facilitate the detection of overfitting by evaluating the model on a dataset it hasn\u2019t been trained on (in short, an overfitting model performs increasingly well on the training set but decreasingly so on the validation set). Of course, every practicioner could generate their own train/validation-split of the data. Public datasets are usually pre-split to simplifiy comparing results between implementations/publications.\n\nEach subfolder has two subsubfolders 3 and 7 which contain the .jpg files for the respective class of images. This is a common way of organizing datasets comprised of pictures. For the full MNIST dataset there are 10 subsubfolders, one for the images for each digit.\"",
    "ground_truth_components": [
      "There are two subfolders, train and valid, the former contains the data for model training, the latter contains the data for validating model performance after each training step",
      "Each subfolder has two subsubfolders 3 and 7",
      "which contain the .jpg files for the respective class of images",
      "Evaluating the model on the validation set",
      "facilitate the detection of overfitting by evaluating the model on a dataset it hasn\u2019t been trained on",
      "a) to report a human-interpretable metric such as accuracy (in contrast to the often abstract loss functions used for training)",
      "Public datasets are usually pre-split to simplifiy comparing results between implementations/publications",
      "For the full MNIST dataset there are 10 subsubfolders, one for the images for each digit"
    ],
    "haiku_components": [
      "There are two subfolders, train and valid",
      "the former contains the data for model training",
      "the latter contains the data for validating model performance after each training step",
      "Evaluating the model on the validation set serves two purposes: a) to report a human-interpretable metric such as accuracy (in contrast to the often abstract loss functions used for training)",
      "b) to facilitate the detection of overfitting by evaluating the model on a dataset it hasn't been trained on (in short, an overfitting model performs increasingly well on the training set but decreasingly so on the validation set)",
      "every practicioner could generate their own train/validation-split of the data",
      "Public datasets are usually pre-split to simplifiy comparing results between implementations/publications",
      "Each subfolder has two subsubfolders 3 and 7 which contain the .jpg files for the respective class of images",
      "This is a common way of organizing datasets comprised of pictures",
      "For the full MNIST dataset there are 10 subsubfolders, one for the images for each digit"
    ],
    "ground_truth_annotations": [
      false,
      false,
      false,
      false,
      false,
      false,
      false,
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        false,
        false,
        false,
        false,
        true,
        false,
        false,
        true
      ],
      "partial": [
        true,
        true,
        true,
        true,
        true,
        false,
        false,
        true,
        false,
        false
      ],
      "extra": [
        false,
        false,
        false,
        false,
        true,
        true,
        false,
        false,
        true,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 3,
    "question_text": "\"Explain how the \"pixel similarity\" approach to classifying digits works.\"",
    "gold_standard_answer": "\"In the \u201cpixel similarity\u201d approach, we generate an archetype for each class we want to identify. In our case, we want to distinguish images of 3\u2019s from images of 7\u2019s. We define the archetypical 3 as the pixel-wise mean value of all 3\u2019s in the training set. Analoguously for the 7\u2019s. You can visualize the two archetypes and see that they are in fact blurred versions of the numbers they represent.\nIn order to tell if a previously unseen image is a 3 or a 7, we calculate its distance to the two archetypes (here: mean pixel-wise absolute difference). We say the new image is a 3 if its distance to the archetypical 3 is lower than two the archetypical 7.\"",
    "ground_truth_components": [
      "We define the archetypical 3 as the pixel-wise mean value of all 3\u2019s in the training set. Analoguously for the 7\u2019s. You can visualize the two archetypes and see that they are in fact blurred versions of the numbers they represent.\nIn order to tell if a previously unseen image is a 3 or a 7, we calculate its distance to the two archetypes (here: mean pixel-wise absolute difference). We say the new image is a 3 if its distance to the archetypical 3 is lower than two the archetypical 7"
    ],
    "haiku_components": [
      "\"In the \"pixel similarity\" approach, we generate an archetype for each class we want to identify.",
      "In our case, we want to distinguish images of 3's from images of 7's.",
      "We define the archetypical 3 as the pixel-wise mean value of all 3's in the training set.",
      "Analoguously for the 7's.",
      "You can visualize the two archetypes and see that they are in fact blurred versions of the numbers they represent.",
      "In order to tell if a previously unseen image is a 3 or a 7, we calculate its distance to the two archetypes (here: mean pixel-wise absolute difference).",
      "We say the new image is a 3 if its distance to the archetypical 3 is lower than two the archetypical 7."
    ],
    "ground_truth_annotations": [
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ],
      "partial": [
        false,
        false,
        true,
        true,
        true,
        true,
        true
      ],
      "extra": [
        true,
        true,
        false,
        false,
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 4,
    "question_text": "\"What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\"",
    "gold_standard_answer": "\"Lists (arrays in other programming languages) are often generated using a for-loop. A list comprehension is a Pythonic way of condensing the creation of a list using a for-loop into a single expression. List comprehensions will also often include if clauses for filtering.\n\nlst_in = range(10)\nlst_out = [2*el for el in lst_in if el%2==1]\n# is equivalent to:\nlst_out = []\nfor el in lst_in:\n   if el%2==1:\n       lst_out.append(2*el)\"",
    "ground_truth_components": [
      "\"Lists (arrays in other programming languages) are often generated using a for-loop. A list comprehension is a Pythonic way of condensing the creation of a list using a for-loop into a single expression. List comprehensions will also often include if clauses for filtering.\n\nlst_in = range(10)\nlst_out = [2*el for el in lst_in if el%2==1]\n# is equivalent to:\nlst_out = []\nfor el in lst_in:\n   if el%2==1:\n       lst_out.append(2*el)\""
    ],
    "haiku_components": [
      "Lists (arrays in other programming languages) are often generated using a for-loop.",
      "A list comprehension is a Pythonic way of condensing the creation of a list using a for-loop into a single expression.",
      "List comprehensions will also often include if clauses for filtering."
    ],
    "ground_truth_annotations": [
      true
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        false
      ],
      "partial": [
        true,
        true,
        true
      ],
      "extra": [
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 5,
    "question_text": "\"What is a \"rank-3 tensor\"?\"",
    "gold_standard_answer": "\"The rank of a tensor is the number of dimensions it has. An easy way to identify the rank is the number of indices you would need to reference a number within a tensor. A scalar can be represented as a tensor of rank 0 (no index), a vector can be represented as a tensor of rank 1 (one index, e.g., v[i]), a matrix can be represented as a tensor of rank 2 (two indices, e.g., a[i,j]), and a tensor of rank 3 is a cuboid or a \u201cstack of matrices\u201d (three indices, e.g., b[i,j,k]). In particular, the rank of a tensor is independent of its shape or dimensionality, e.g., a tensor of shape 2x2x2 and a tensor of shape 3x5x7 both have rank 3.\nNote that the term \u201crank\u201d has different meanings in the context of tensors and matrices (where it refers to the number of linearly independent column vectors).\"",
    "ground_truth_components": [
      "The rank of a tensor is the number of dimensions it has",
      "An easy way to identify the rank is the number of indices you would need to reference a number within a tensor",
      "A scalar can be represented as a tensor of rank 0 (no index), a vector can be represented as a tensor of rank 1 (one index, e.g., v[i]), a matrix can be represented as a tensor of rank 2 (two indices, e.g., a[i,j])",
      "and a tensor of rank 3 is a cuboid or a \u201cstack of matrices\u201d",
      "In particular, the rank of a tensor is independent of its shape or dimensionality, e.g., a tensor of shape 2x2x2 and a tensor of shape 3x5x7 both have rank 3",
      "Note that the term \u201crank\u201d has different meanings in the context of tensors and matrices (where it refers to the number of linearly independent column vectors)"
    ],
    "haiku_components": [
      "The rank of a tensor is the number of dimensions it has.",
      "An easy way to identify the rank is the number of indices you would need to reference a number within a tensor.",
      "A scalar can be represented as a tensor of rank 0 (no index)",
      "A vector can be represented as a tensor of rank 1 (one index, e.g., v[i])",
      "A matrix can be represented as a tensor of rank 2 (two indices, e.g., a[i,j])",
      "A tensor of rank 3 is a cuboid or a \"stack of matrices\" (three indices, e.g., b[i,j,k])",
      "The rank of a tensor is independent of its shape or dimensionality, e.g., a tensor of shape 2x2x2 and a tensor of shape 3x5x7 both have rank 3.",
      "The term \"rank\" has different meanings in the context of tensors and matrices (where it refers to the number of linearly independent column vectors)."
    ],
    "ground_truth_annotations": [
      false,
      false,
      false,
      false,
      false,
      false
    ],
    "haiku_annotations": {
      "exact": [
        true,
        true,
        false,
        false,
        false,
        false,
        true,
        true
      ],
      "partial": [
        false,
        false,
        true,
        true,
        true,
        true,
        false,
        false
      ],
      "extra": [
        false,
        false,
        false,
        false,
        false,
        true,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 6,
    "question_text": "\"What is the difference between tensor rank and shape? How do you get the rank from the shape?\"",
    "gold_standard_answer": "\"Rank is the number of axes or dimensions in a tensor; shape is the size of each axis of a tensor.\n\nThe length of a tensor\u2019s shape is its rank.\n\nSo if we have the images of the 3 folder from the MINST_SAMPLE dataset in a tensor called stacked_threes and we find its shape like this.\n\nIn [ ]: stacked_threes.shape\nOut[ ]: torch.Size([6131, 28, 28])\nWe just need to find its length to know its rank. This is done as follows.\n\nIn [ ]: len(stacked_threes.shape)\nOut[ ]: 3\nYou can also get a tensor\u2019s rank directly with ndim .\n\nIn [ ]: stacked_threes.ndim\nOut[ ]: 3\"",
    "ground_truth_components": [
      "Rank is the number of axes or dimensions in a tensor",
      "shape is the size of each axis of a tensor",
      "The length of a tensor\u2019s shape is its rank",
      "So if we have the images of the 3 folder from the MINST_SAMPLE dataset in a tensor called stacked_threes and we find its shape like this.\n\nIn [ ]: stacked_threes.shape\nOut[ ]: torch.Size([6131, 28, 28])\n\"",
      "In [ ]: stacked_threes.ndim\nOut[ ]: 3",
      "You can also get a tensor\u2019s rank directly with ndim"
    ],
    "haiku_components": [
      "Rank is the number of axes or dimensions in a tensor;",
      "shape is the size of each axis of a tensor.",
      "The length of a tensor's shape is its rank.",
      "So if we have the images of the 3 folder from the MINST_SAMPLE dataset in a tensor called stacked_threes and we find its shape like this.",
      "We just need to find its length to know its rank. This is done as follows.",
      "You can also get a tensor's rank directly with ndim ."
    ],
    "ground_truth_annotations": [
      false,
      false,
      false,
      true,
      true,
      false
    ],
    "haiku_annotations": {
      "exact": [
        true,
        true,
        true,
        false,
        false,
        true
      ],
      "partial": [
        false,
        false,
        false,
        true,
        false,
        false
      ],
      "extra": [
        false,
        false,
        false,
        false,
        true,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 7,
    "question_text": "\"What are RMSE and L1 norm?\"",
    "gold_standard_answer": "\"Root mean square error (RMSE), also called the L2 norm, and mean absolute difference (MAE), also called the L1 norm, are two commonly used methods of measuring \u201cdistance\u201d. Simple differences do not work because some difference are positive and others are negative, canceling each other out. Therefore, a function that focuses on the magnitudes of the differences is needed to properly measure distances. The simplest would be to add the absolute values of the differences, which is what MAE is. RMSE takes the mean of the square (makes everything positive) and then takes the square root (undoes squaring).\"",
    "ground_truth_components": [
      [
        "Root mean square error (RMSE), also called the L2 norm, and mean absolute difference (MAE), also called the L1 norm",
        "The simplest would be to add the absolute values of the differences, which is what MAE is. RMSE takes the mean of the square (makes everything positive) and then takes the square root (undoes squaring)."
      ],
      "two commonly used methods of measuring \u201cdistance\u201d",
      "Simple differences do not work because some difference are positive and others are negative, canceling each other out. Therefore, a function that focuses on the magnitudes of the differences is needed to properly measure distances"
    ],
    "haiku_components": [
      "Root mean square error (RMSE), also called the L2 norm",
      "mean absolute difference (MAE), also called the L1 norm",
      "two commonly used methods of measuring \"distance\"",
      "Simple differences do not work because some difference are positive and others are negative, canceling each other out",
      "a function that focuses on the magnitudes of the differences is needed to properly measure distances",
      "The simplest would be to add the absolute values of the differences, which is what MAE is",
      "RMSE takes the mean of the square (makes everything positive) and then takes the square root (undoes squaring)"
    ],
    "ground_truth_annotations": [
      false,
      false,
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        true,
        false,
        false,
        false,
        false
      ],
      "partial": [
        true,
        true,
        false,
        true,
        true,
        true,
        true
      ],
      "extra": [
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 8,
    "question_text": "\"How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?\"",
    "gold_standard_answer": "\"As loops are very slow in Python, it is best to represent the operations as array operations rather than looping through individual elements. If this can be done, then using NumPy or PyTorch will be thousands of times faster, as they use underlying C code which is much faster than pure Python. Even better, PyTorch allows you to run operations on GPU, which will have significant speedup if there are parallel operations that can be done.\"",
    "ground_truth_components": [
      "As loops are very slow in Python, it is best to represent the operations as array operations rather than looping through individual elements. If this can be done, then using NumPy or PyTorch will be thousands of times faster, as they use underlying C code which is much faster than pure Python",
      "Even better, PyTorch allows you to run operations on GPU, which will have significant speedup if there are parallel operations that can be done"
    ],
    "haiku_components": [
      "loops are very slow in Python",
      "represent the operations as array operations rather than looping through individual elements",
      "using NumPy or PyTorch will be thousands of times faster",
      "they use underlying C code which is much faster than pure Python",
      "PyTorch allows you to run operations on GPU",
      "GPU operations will have significant speedup if there are parallel operations that can be done"
    ],
    "ground_truth_annotations": [
      false,
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        false,
        false,
        false,
        false
      ],
      "partial": [
        true,
        true,
        true,
        true,
        true,
        true
      ],
      "extra": [
        false,
        false,
        false,
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false,
        false,
        true
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 10,
    "question_text": "\"What is broadcasting?\"",
    "gold_standard_answer": "\"Scientific/numerical Python packages like NumPy and PyTorch will often implement broadcasting that often makes code easier to write. In the case of PyTorch, tensors with smaller rank are expanded to have the same size as the larger rank tensor. In this way, operations can be performed between tensors with different rank.\"",
    "ground_truth_components": [
      "\"Scientific/numerical Python packages like NumPy and PyTorch will often implement broadcasting that often makes code easier to write. In the case of PyTorch, tensors with smaller rank are expanded to have the same size as the larger rank tensor. In this way, operations can be performed between tensors with different rank.\""
    ],
    "haiku_components": [
      "Scientific/numerical Python packages like NumPy and PyTorch",
      "broadcasting",
      "broadcasting often makes code easier to write",
      "In the case of PyTorch, tensors with smaller rank are expanded to have the same size as the larger rank tensor",
      "operations can be performed between tensors with different rank"
    ],
    "ground_truth_annotations": [
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        false,
        false,
        false
      ],
      "partial": [
        true,
        true,
        true,
        true,
        true
      ],
      "extra": [
        false,
        false,
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 11,
    "question_text": "\"Are metrics generally calculated using the training set, or the validation set? Why?\"",
    "gold_standard_answer": "\"Metrics are generally calculated on a validation set. As the validation set is unseen data for the model, evaluating the metrics on the validation set is better in order to determine if there is any overfitting and how well the model might generalize if given similar data.\"",
    "ground_truth_components": [
      "Metrics are generally calculated on a validation set",
      "As the validation set is unseen data for the model, evaluating the metrics on the validation set is better in order to determine if there is any overfitting",
      "how well the model might generalize if given similar data"
    ],
    "haiku_components": [
      "Metrics are generally calculated on a validation set.",
      "As the validation set is unseen data for the model, evaluating the metrics on the validation set is better in order to determine if there is any overfitting",
      "and how well the model might generalize if given similar data."
    ],
    "ground_truth_annotations": [
      false,
      false,
      false
    ],
    "haiku_annotations": {
      "exact": [
        true,
        true,
        true
      ],
      "partial": [
        false,
        false,
        false
      ],
      "extra": [
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 12,
    "question_text": "\"What is SGD?\"",
    "gold_standard_answer": "\"SGD, or stochastic gradient descent, is an optimization algorithm. Specifically, SGD is an algorithm that will update the parameters of a model in order to minimize a given loss function that was evaluated on the predictions and target. The key idea behind SGD (and many optimization algorithms, for that matter) is that the gradient of the loss function provides an indication of how that loss function changes in the parameter space, which we can use to determine how best to update the parameters in order to minimize the loss function. This is what SGD does.\"",
    "ground_truth_components": [
      "SGD, or stochastic gradient descent,",
      "optimization algorithm",
      "SGD is an algorithm that will update the parameters of a model",
      "in order to minimize a given loss function that was evaluated on the predictions and target",
      "The key idea behind SGD (and many optimization algorithms, for that matter) is that the gradient of the loss function provides an indication of how that loss function changes in the parameter space, which we can use to determine how best to update the parameters in order to minimize the loss function"
    ],
    "haiku_components": [
      "SGD, or stochastic gradient descent, is an optimization algorithm.",
      "SGD is an algorithm that will update the parameters of a model in order to minimize a given loss function that was evaluated on the predictions and target.",
      "The key idea behind SGD (and many optimization algorithms, for that matter) is that the gradient of the loss function provides an indication of how that loss function changes in the parameter space, which we can use to determine how best to update the parameters in order to minimize the loss function.",
      "This is what SGD does."
    ],
    "ground_truth_annotations": [
      false,
      false,
      false,
      false,
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        true,
        false
      ],
      "partial": [
        true,
        true,
        false,
        false
      ],
      "extra": [
        false,
        false,
        false,
        true
      ],
      "hallucination": [
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 13,
    "question_text": "\"Why does SGD use mini-batches?\"",
    "gold_standard_answer": "\"We need to calculate our loss function (and our gradient) on one or more data points. We cannot calculate on the whole datasets due to compute limitations and time constraints. If we iterated through each data point, however, the gradient will be unstable and imprecise, and is not suitable for training. As a compromise, we calculate the average loss for a small subset of the dataset at a time. This subset is called a mini-batch. Using mini-batches are also more computationally efficient than single items on a GPU.\"",
    "ground_truth_components": [
      "We need to calculate our loss function (and our gradient) on one or more data points. We cannot calculate on the whole datasets due to compute limitations and time constraints. If we iterated through each data point, however, the gradient will be unstable and imprecise, and is not suitable for training",
      "As a compromise, we calculate the average loss for a small subset of the dataset at a time. This subset is called a mini-batch",
      "Using mini-batches are also more computationally efficient than single items on a GPU"
    ],
    "haiku_components": [
      "We need to calculate our loss function (and our gradient) on one or more data points.",
      "We cannot calculate on the whole datasets due to compute limitations and time constraints.",
      "If we iterated through each data point, however, the gradient will be unstable and imprecise, and is not suitable for training.",
      "As a compromise, we calculate the average loss for a small subset of the dataset at a time.",
      "This subset is called a mini-batch.",
      "Using mini-batches are also more computationally efficient than single items on a GPU."
    ],
    "ground_truth_annotations": [
      false,
      false,
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        false,
        false,
        false,
        true
      ],
      "partial": [
        true,
        true,
        true,
        true,
        true,
        false
      ],
      "extra": [
        false,
        false,
        false,
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 14,
    "question_text": "\"What are the seven steps in SGD for machine learning?\"",
    "gold_standard_answer": "\"Initialize the parameters \u2013 Random values often work best.\nCalculate the predictions \u2013 This is done on the training set, one mini-batch at a time.\nCalculate the loss \u2013 The average loss over the minibatch is calculated\nCalculate the gradients \u2013 this is an approximation of how the parameters need to change in order to minimize the loss function\nStep the weights \u2013 update the parameters based on the calculated weights\nRepeat the process\nStop \u2013 In practice, this is either based on time constraints or usually based on when the training/validation losses and metrics stop improving.\"",
    "ground_truth_components": [
      "\"Initialize the parameters \u2013 Random values often work best.\nCalculate the predictions \u2013 This is done on the training set, one mini-batch at a time.\nCalculate the loss \u2013 The average loss over the minibatch is calculated\nCalculate the gradients \u2013 this is an approximation of how the parameters need to change in order to minimize the loss function\nStep the weights \u2013 update the parameters based on the calculated weights\nRepeat the process\nStop \u2013 In practice, this is either based on time constraints or usually based on when the training/validation losses and metrics stop improving.\"",
      "\"Initialize the parameters \u2013 Random values often work best.\nCalculate the predictions \u2013 This is done on the training set, one mini-batch at a time.\nCalculate the loss \u2013 The average loss over the minibatch is calculated\nCalculate the gradients \u2013 this is an approximation of how the parameters need to change in order to minimize the loss function\nStep the weights \u2013 update the parameters based on the calculated weights\nRepeat the process\nStop \u2013 In practice, this is either based on time constraints or usually based on when the training/validation losses and metrics stop improving.\""
    ],
    "haiku_components": [
      "Initialize the parameters \u2013 Random values often work best.",
      "Calculate the predictions \u2013 This is done on the training set, one mini-batch at a time.",
      "Calculate the loss \u2013 The average loss over the minibatch is calculated",
      "Calculate the gradients \u2013 this is an approximation of how the parameters need to change in order to minimize the loss function",
      "Step the weights \u2013 update the parameters based on the calculated weights",
      "Repeat the process",
      "Stop \u2013 In practice, this is either based on time constraints or usually based on when the training/validation losses and metrics stop improving."
    ],
    "ground_truth_annotations": [
      false,
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ],
      "partial": [
        true,
        true,
        true,
        true,
        true,
        true,
        true
      ],
      "extra": [
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 15,
    "question_text": "\"How do we initialize the weights in a model?\"",
    "gold_standard_answer": "\"Random weights work pretty well.\"",
    "ground_truth_components": [
      "\"Random weights work pretty well.\""
    ],
    "haiku_components": [
      "\"Random weights\"",
      "\"work pretty well\""
    ],
    "ground_truth_annotations": [
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false
      ],
      "partial": [
        true,
        true
      ],
      "extra": [
        false,
        false
      ],
      "hallucination": [
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 16,
    "question_text": "\"What is \"loss\"?\"",
    "gold_standard_answer": "\"The loss function will return a value based on the given predictions and targets, where lower values correspond to better model predictions.\"",
    "ground_truth_components": [
      "The loss function will return a value based on the given predictions and targets",
      "lower values correspond to better model predictions"
    ],
    "haiku_components": [
      "The loss function",
      "will return a value",
      "based on the given predictions and targets",
      "lower values correspond to better model predictions"
    ],
    "ground_truth_annotations": [
      false,
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        false,
        true
      ],
      "partial": [
        true,
        true,
        true,
        false
      ],
      "extra": [
        false,
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 17,
    "question_text": "\"Why can't we always use a high learning rate?\"",
    "gold_standard_answer": "\"The loss may \u201cbounce\u201d around (oscillate) or even diverge, as the optimizer is taking steps that are too large, and updating the parameters faster than it should be.\"",
    "ground_truth_components": [
      "or even diverge",
      "The loss may \u201cbounce\u201d around (oscillate)",
      "the optimizer is taking steps that are too large, and updating the parameters faster than it should be"
    ],
    "haiku_components": [
      "\"The loss may \"bounce\" around (oscillate)\"",
      "\"or even diverge\"",
      "\"as the optimizer is taking steps that are too large\"",
      "\"and updating the parameters faster than it should be.\""
    ],
    "ground_truth_annotations": [
      false,
      false,
      false
    ],
    "haiku_annotations": {
      "exact": [
        true,
        true,
        false,
        false
      ],
      "partial": [
        false,
        false,
        true,
        true
      ],
      "extra": [
        false,
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 18,
    "question_text": "\"What is a \"gradient\"?\"",
    "gold_standard_answer": "\"The gradients tell us how much we have to change each weight to make our model better. It is essentially a measure of how the loss function changes with changes of the weights of the model (the derivative).\"",
    "ground_truth_components": [
      "The gradients tell us how much we have to change each weight to make our model better",
      "It is essentially a measure of how the loss function changes with changes of the weights of the model (the derivative)"
    ],
    "haiku_components": [
      "The gradients tell us how much we have to change each weight to make our model better.",
      "It is essentially a measure of how the loss function changes with changes of the weights of the model (the derivative)."
    ],
    "ground_truth_annotations": [
      false,
      false
    ],
    "haiku_annotations": {
      "exact": [
        true,
        true
      ],
      "partial": [
        false,
        false
      ],
      "extra": [
        false,
        false
      ],
      "hallucination": [
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 19,
    "question_text": "\"Do you need to know how to calculate gradients yourself?\"",
    "gold_standard_answer": "\"Manual calculation of the gradients are not required, as deep learning libraries will automatically calculate the gradients for you. This feature is known as automatic differentiation. In PyTorch, if requires_grad=True, the gradients can be returned by calling the backward method: a.backward()\"",
    "ground_truth_components": [
      "Manual calculation of the gradients are not required, as deep learning libraries will automatically calculate the gradients for you",
      "In PyTorch, if requires_grad=True, the gradients can be returned by calling the backward method: a.backward()",
      "This feature is known as automatic differentiation"
    ],
    "haiku_components": [
      "Manual calculation of the gradients are not required",
      "deep learning libraries will automatically calculate the gradients for you",
      "This feature is known as automatic differentiation",
      "In PyTorch, if requires_grad=True, the gradients can be returned by calling the backward method: a.backward()"
    ],
    "ground_truth_annotations": [
      false,
      false,
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        true,
        true
      ],
      "partial": [
        true,
        true,
        false,
        false
      ],
      "extra": [
        false,
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 20,
    "question_text": "\"Why can't we use accuracy as a loss function?\"",
    "gold_standard_answer": "\"A loss function needs to change as the weights are being adjusted. Accuracy only changes if the predictions of the model change. So if there are slight changes to the model that, say, improves confidence in a prediction, but does not change the prediction, the accuracy will still not change. Therefore, the gradients will be zero everywhere except when the actual predictions change. The model therefore cannot learn from the gradients equal to zero, and the model\u2019s weights will not update and will not train. A good loss function gives a slightly better loss when the model gives slightly better predictions. Slightly better predictions mean if the model is more confident about the correct prediction. For example, predicting 0.9 vs 0.7 for probability that a MNIST image is a 3 would be slightly better prediction. The loss function needs to reflect that.\"",
    "ground_truth_components": [
      "A loss function needs to change as the weights are being adjusted",
      "Accuracy only changes if the predictions of the model change. So if there are slight changes to the model that, say, improves confidence in a prediction, but does not change the prediction, the accuracy will still not change. Therefore, the gradients will be zero everywhere except when the actual predictions change",
      "The model therefore cannot learn from the gradients equal to zero, and the model\u2019s weights will not update and will not train",
      "A good loss function gives a slightly better loss when the model gives slightly better predictions",
      "Slightly better predictions mean if the model is more confident about the correct prediction"
    ],
    "haiku_components": [
      "A loss function needs to change as the weights are being adjusted.",
      "Accuracy only changes if the predictions of the model change.",
      "If there are slight changes to the model that, say, improves confidence in a prediction, but does not change the prediction, the accuracy will still not change.",
      "The gradients will be zero everywhere except when the actual predictions change.",
      "The model cannot learn from the gradients equal to zero, and the model's weights will not update and will not train.",
      "A good loss function gives a slightly better loss when the model gives slightly better predictions.",
      "Slightly better predictions mean if the model is more confident about the correct prediction.",
      "For example, predicting 0.9 vs 0.7 for probability that a MNIST image is a 3 would be slightly better prediction.",
      "The loss function needs to reflect that."
    ],
    "ground_truth_annotations": [
      false,
      false,
      false,
      false,
      false
    ],
    "haiku_annotations": {
      "exact": [
        true,
        false,
        false,
        false,
        true,
        true,
        true,
        false,
        false
      ],
      "partial": [
        false,
        true,
        true,
        true,
        false,
        false,
        false,
        false,
        false
      ],
      "extra": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        true,
        true
      ],
      "hallucination": [
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 22,
    "question_text": "\"What is the difference between a loss function and a metric?\"",
    "gold_standard_answer": "\"The key difference is that metrics drive human understanding and losses drive automated learning. In order for loss to be useful for training, it needs to have a meaningful derivative. Many metrics, like accuracy are not like that. Metrics instead are the numbers that humans care about, that reflect the performance of the model.\"",
    "ground_truth_components": [
      "\"The key difference is that metrics drive human understanding and losses drive automated learning. In order for loss to be useful for training, it needs to have a meaningful derivative. Many metrics, like accuracy are not like that. Metrics instead are the numbers that humans care about, that reflect the performance of the model.\""
    ],
    "haiku_components": [
      "The key difference is that metrics drive human understanding",
      "losses drive automated learning",
      "In order for loss to be useful for training, it needs to have a meaningful derivative",
      "Many metrics, like accuracy are not like that",
      "Metrics instead are the numbers that humans care about, that reflect the performance of the model"
    ],
    "ground_truth_annotations": [
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        false,
        false,
        false
      ],
      "partial": [
        true,
        true,
        true,
        true,
        true
      ],
      "extra": [
        false,
        false,
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 23,
    "question_text": "\"What is the function to calculate new weights using a learning rate?\"",
    "gold_standard_answer": "\"The optimizer step function\"",
    "ground_truth_components": [
      "\"The optimizer step function\""
    ],
    "haiku_components": [
      "The optimizer step function"
    ],
    "ground_truth_annotations": [
      false
    ],
    "haiku_annotations": {
      "exact": [
        true
      ],
      "partial": [
        false
      ],
      "extra": [
        false
      ],
      "hallucination": [
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 24,
    "question_text": "\"What does the DataLoader class do?\"",
    "gold_standard_answer": "\"The DataLoader class can take any Python collection and turn it into an iterator over many batches.\"",
    "ground_truth_components": [
      "\"The DataLoader class can take any Python collection and turn it into an iterator over many batches.\""
    ],
    "haiku_components": [
      "The DataLoader class",
      "can take any Python collection",
      "turn it into an iterator over many batches"
    ],
    "ground_truth_annotations": [
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        false
      ],
      "partial": [
        true,
        true,
        true
      ],
      "extra": [
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 27,
    "question_text": "\"What does view do in PyTorch?\"",
    "gold_standard_answer": "\"It changes the shape of a Tensor without changing its contents.\"",
    "ground_truth_components": [
      "\"It changes the shape of a Tensor without changing its contents.\""
    ],
    "haiku_components": [
      "It changes the shape of a Tensor",
      "without changing its contents"
    ],
    "ground_truth_annotations": [
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false
      ],
      "partial": [
        true,
        true
      ],
      "extra": [
        false,
        false
      ],
      "hallucination": [
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 28,
    "question_text": "\"What are the \"bias\" parameters in a neural network? Why do we need them?\"",
    "gold_standard_answer": "\"Without the bias parameters, if the input is zero, the output will always be zero. Therefore, using bias parameters adds additional flexibility to the model.\"",
    "ground_truth_components": [
      "\"Without the bias parameters, if the input is zero, the output will always be zero. Therefore, using bias parameters adds additional flexibility to the model.\""
    ],
    "haiku_components": [
      "Without the bias parameters, if the input is zero, the output will always be zero.",
      "Therefore, using bias parameters adds additional flexibility to the model."
    ],
    "ground_truth_annotations": [
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false
      ],
      "partial": [
        true,
        true
      ],
      "extra": [
        false,
        false
      ],
      "hallucination": [
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 29,
    "question_text": "\"What does the @ operator do in Python?\"",
    "gold_standard_answer": "\"This is the matrix multiplication operator.\"",
    "ground_truth_components": [
      "\"This is the matrix multiplication operator.\""
    ],
    "haiku_components": [
      "This is the matrix multiplication operator."
    ],
    "ground_truth_annotations": [
      false
    ],
    "haiku_annotations": {
      "exact": [
        true
      ],
      "partial": [
        false
      ],
      "extra": [
        false
      ],
      "hallucination": [
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 30,
    "question_text": "\"What does the backward method do?\"",
    "gold_standard_answer": "\"This method returns the current gradients.\"",
    "ground_truth_components": [
      "\"This method returns the current gradients.\""
    ],
    "haiku_components": [
      "This method",
      "returns",
      "the current gradients"
    ],
    "ground_truth_annotations": [
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        false
      ],
      "partial": [
        true,
        true,
        true
      ],
      "extra": [
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 31,
    "question_text": "\"Why do we have to zero the gradients?\"",
    "gold_standard_answer": "\"PyTorch will add the gradients of a variable to any previously stored gradients. If the training loop function is called multiple times, without zeroing the gradients, the gradient of current loss would be added to the previously stored gradient value.\"",
    "ground_truth_components": [
      "PyTorch will add the gradients of a variable to any previously stored gradients. If the training loop function is called multiple times, without zeroing the gradients, the gradient of current loss would be added to the previously stored gradient value"
    ],
    "haiku_components": [
      "PyTorch will add the gradients of a variable to any previously stored gradients.",
      "If the training loop function is called multiple times, without zeroing the gradients, the gradient of current loss would be added to the previously stored gradient value."
    ],
    "ground_truth_annotations": [
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false
      ],
      "partial": [
        true,
        true
      ],
      "extra": [
        false,
        false
      ],
      "hallucination": [
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 32,
    "question_text": "\"What information do we have to pass to Learner?\"",
    "gold_standard_answer": "\"We need to pass in the DataLoaders, the model, the optimization function, the loss function, and optionally any metrics to print.\"",
    "ground_truth_components": [
      "\"We need to pass in the DataLoaders, the model, the optimization function, the loss function, and optionally any metrics to print.\""
    ],
    "haiku_components": [
      "DataLoaders",
      "model",
      "optimization function",
      "loss function",
      "metrics"
    ],
    "ground_truth_annotations": [
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        false,
        false,
        false
      ],
      "partial": [
        true,
        true,
        true,
        true,
        true
      ],
      "extra": [
        false,
        false,
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 34,
    "question_text": "\"What is \"ReLU\"? Draw a plot of it for values from -2 to +2.\"",
    "gold_standard_answer": "\"ReLU just means \u201creplace any negative numbers with zero\u201d. It is a commonly used activation function.\"",
    "ground_truth_components": [
      "\"ReLU just means \u201creplace any negative numbers with zero\u201d. It is a commonly used activation function.\""
    ],
    "haiku_components": [
      "\"ReLU just means \"replace any negative numbers with zero\"\"",
      "\"It is a commonly used activation function.\""
    ],
    "ground_truth_annotations": [
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false
      ],
      "partial": [
        true,
        true
      ],
      "extra": [
        false,
        false
      ],
      "hallucination": [
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 35,
    "question_text": "\"What is an \"activation function\"?\"",
    "gold_standard_answer": "\"The activation function is another function that is part of the neural network, which has the purpose of providing non-linearity to the model. The idea is that without an activation function, we just have multiple linear functions of the form y=mx+b. However, a series of linear layers is equivalent to a single linear layer, so our model can only fit a line to the data. By introducing a non-linearity in between the linear layers, this is no longer true. Each layer is somewhat decoupled from the rest of the layers, and the model can now fit much more complex functions. In fact, it can be mathematically proven that such a model can solve any computable problem to an arbitrarily high accuracy, if the model is large enough with the correct weights. This is known as the universal approximation theorem.\"",
    "ground_truth_components": [
      "The activation function is another function that is part of the neural network, which has the purpose of providing non-linearity to the model",
      "a series of linear layers is equivalent to a single linear layer, so our model can only fit a line to the data",
      "By introducing a non-linearity in between the linear layers, this is no longer true. Each layer is somewhat decoupled from the rest of the layers, and the model can now fit much more complex functions",
      "In fact, it can be mathematically proven that such a model can solve any computable problem to an arbitrarily high accuracy, if the model is large enough with the correct weights. This is known as the universal approximation theorem"
    ],
    "haiku_components": [
      "The activation function is another function that is part of the neural network, which has the purpose of providing non-linearity to the model.",
      "Without an activation function, we just have multiple linear functions of the form y=mx+b.",
      "A series of linear layers is equivalent to a single linear layer, so our model can only fit a line to the data.",
      "By introducing a non-linearity in between the linear layers, this is no longer true.",
      "Each layer is somewhat decoupled from the rest of the layers, and the model can now fit much more complex functions.",
      "It can be mathematically proven that such a model can solve any computable problem to an arbitrarily high accuracy, if the model is large enough with the correct weights.",
      "This is known as the universal approximation theorem."
    ],
    "ground_truth_annotations": [
      false,
      false,
      false,
      false
    ],
    "haiku_annotations": {
      "exact": [
        true,
        false,
        true,
        false,
        false,
        false,
        false
      ],
      "partial": [
        false,
        false,
        false,
        true,
        true,
        true,
        true
      ],
      "extra": [
        false,
        true,
        false,
        false,
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false,
        false,
        false,
        false
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 36,
    "question_text": "\"What's the difference between F.relu and nn.ReLU?\"",
    "gold_standard_answer": "\"F.relu is a Python function for the relu activation function. On the other hand, nn.ReLU is a PyTorch module. This means that it is a Python class that can be called as a function in the same way as F.relu.\"",
    "ground_truth_components": [
      "\"F.relu is a Python function for the relu activation function. On the other hand, nn.ReLU is a PyTorch module. This means that it is a Python class that can be called as a function in the same way as F.relu.\""
    ],
    "haiku_components": [
      "F.relu is a Python function for the relu activation function.",
      "nn.ReLU is a PyTorch module.",
      "nn.ReLU is a Python class that can be called as a function in the same way as F.relu."
    ],
    "ground_truth_annotations": [
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        false
      ],
      "partial": [
        true,
        true,
        true
      ],
      "extra": [
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        true
      ]
    }
  },
  {
    "chapter": 4,
    "question_number": 37,
    "question_text": "\"The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?\"",
    "gold_standard_answer": "\"There are practical performance benefits to using more than one nonlinearity. We can use a deeper model with less number of parameters, better performance, faster training, and less compute/memory requirements.\"",
    "ground_truth_components": [
      "\"There are practical performance benefits to using more than one nonlinearity. We can use a deeper model with less number of parameters, better performance, faster training, and less compute/memory requirements.\""
    ],
    "haiku_components": [
      "There are practical performance benefits to using more than one nonlinearity.",
      "We can use a deeper model with less number of parameters",
      "better performance",
      "faster training",
      "less compute/memory requirements"
    ],
    "ground_truth_annotations": [
      false
    ],
    "haiku_annotations": {
      "exact": [
        false,
        false,
        false,
        false,
        false
      ],
      "partial": [
        true,
        true,
        true,
        true,
        true
      ],
      "extra": [
        false,
        false,
        false,
        false,
        false
      ],
      "hallucination": [
        false,
        false,
        false,
        false,
        false
      ]
    }
  }
]